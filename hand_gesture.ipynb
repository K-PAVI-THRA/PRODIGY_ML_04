import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint

# Define the path to the dataset (update with the correct path on your machine)
dataset_path = '/path/to/leapGestRecog/'

# Parameters for image preprocessing
img_size = (64, 64)  # Resize images to 64x64
batch_size = 32

# Image Preprocessing
def load_data(dataset_path):
    images = []
    labels = []
    classes = os.listdir(dataset_path)  # List of gesture classes (folders)
    
    for label, class_name in enumerate(classes):
        class_path = os.path.join(dataset_path, class_name)
        
        if os.path.isdir(class_path):
            for img_name in os.listdir(class_path):
                img_path = os.path.join(class_path, img_name)
                
                # Load and preprocess the image
                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                img = cv2.resize(img, img_size)  # Resize to 64x64
                images.append(img)
                labels.append(label)
    
    # Convert to numpy arrays
    images = np.array(images)
    labels = np.array(labels)
    
    # Normalize pixel values
    images = images.astype('float32') / 255.0
    
    # Expand dimensions for CNN (height, width, channels)
    images = np.expand_dims(images, axis=-1)
    
    return images, labels

# Load the dataset
images, labels = load_data(dataset_path)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# Data Augmentation (to prevent overfitting)
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
datagen.fit(X_train)

# Define the CNN Model
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(os.listdir(dataset_path)), activation='softmax')  # Output layer for classification
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define a checkpoint to save the best model
checkpoint = ModelCheckpoint('gesture_recognition_model.h5', save_best_only=True, monitor='val_loss', mode='min')

# Train the model
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=batch_size),
    epochs=10,
    validation_data=(X_val, y_val),
    callbacks=[checkpoint]
)

# Plot the training and validation accuracy
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Loss: {loss}')
print(f'Validation Accuracy: {accuracy}')

# Save the model (in case you want to use it later)
model.save('gesture_recognition_model_final.h5')

# Function to make predictions on new images
def predict_gesture(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, img_size)  # Resize to 64x64
    img = np.expand_dims(img, axis=-1)  # Add channel dimension
    img = np.expand_dims(img, axis=0)  # Add batch dimension
    img = img.astype('float32') / 255.0  # Normalize
    
    prediction = model.predict(img)
    predicted_class = np.argmax(prediction)
    return predicted_class

# Example of predicting a gesture
test_image = '/path/to/test/image.jpg'
predicted_class = predict_gesture(test_image)
print(f'Predicted Gesture Class: {predicted_class}')
